"""
HICO-DET Grounding Evaluation Script for SFT-trained Qwen3VL with Tool Use

Evaluates a SFT-trained Qwen3VL checkpoint on HICO-DET grounding using
a vLLM server and a custom multi-turn tool-call loop.

The SFT model was trained on Chain-of-Focus (CoF) trajectories with:
- zoom_in(bbox_2d, target_image) tool for region inspection
- zoom_out() tool to return to full view
- <answer>...</answer> tags for final output

Metrics: COCO-style Average Recall @ IoU thresholds (same as baseline).
Does NOT include person-person interactions (HICO-DET).
"""

import os
import json
import re
import argparse
import base64
import io
import math
from tqdm import tqdm
from collections import defaultdict
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont
import numpy as np

from openai import OpenAI

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False

# =============================================================================
# System Prompt (must match SFT training exactly)
# =============================================================================
SYSTEM_PROMPT = """\
You are an expert at analyzing human-object interactions in images. You have \
access to visual tools to help you inspect details when needed.

Your task is to analyze the spatial relationships and interactions between \
people and objects in an image. You will be provided with an image and object \
detection proposals (bounding boxes with class labels and confidence scores).

## Available Tools

You have access to two tools to help you analyze the image:

- **zoom_in(bbox_2d)**: Crop and zoom into a specific region of the image \
for closer inspection. The bbox_2d parameter should be a bounding box in the \
format [x1, y1, x2, y2] using 1000x1000 normalized coordinates, where \
(x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.

- **zoom_out()**: Return to the full original image view after zooming in. \
This takes no parameters.

## When to Use Tools

- Use zoom_in when the interaction between a person and object is ambiguous, \
when objects are small or far away, or when you need to verify fine-grained \
contact details (e.g., is the person actually holding the object, or just \
standing near it?).

- Do NOT use tools when the interaction is clearly visible from the full \
image and proposals alone.

- You may use tools multiple times if needed. For example, if the first zoom \
doesn't clarify things, you can zoom into a different region or zoom in closer.

- When multiple candidate pairs exist, consider zooming into each region \
separately to verify each pair rather than making assumptions from a single view.

- After zooming in, use zoom_out to return to the full image before zooming \
into a different region. This helps you maintain spatial context.

## Verifying Your Initial Assessment

- If a zoom reveals something unexpected (e.g., a person you assumed was \
interacting turns out to be a bystander, or an object you thought was one \
thing is actually another), acknowledge the new evidence and revise your \
assessment accordingly. Do not force-fit your initial hypothesis.

- Proposals can be misleading — a detected "person" near an object does not \
necessarily mean they are interacting. Always verify through visual inspection, \
especially for actions that require specific positioning (driving, riding, \
carrying, etc.).

- If a zoom doesn't help clarify the situation, acknowledge that and either \
try a different region or make your best judgment based on available information.

## Important Notes on Proposals

The candidate proposals are generated by an object detector and may be \
incomplete — not every person or object in the image will necessarily have a \
proposal. If you observe additional people or objects in the image that are \
relevant to the task but missing from the proposals, you should still include \
them in your answer. Estimate their bounding boxes in 1000x1000 normalized \
coordinates based on your visual inspection.

Conversely, proposals can be wrong — a detection may have the incorrect label, \
or a bounding box may capture the wrong region. Treat proposals as suggestions, \
not ground truth. Your visual inspection takes priority.

## Response Format

Follow this process:

1. First, reason about the image and proposals in a <think> block. \
Explain what you observe in the full image, what spatial relationships you \
can identify, what aspects are ambiguous or unclear, and whether you need to \
zoom in to verify anything.

2. If you determine you need to zoom in, call the zoom_in tool with the \
appropriate bounding box coordinates. After receiving the zoomed result, \
reason again in another <think> block about what the zoomed view reveals.

3. You may zoom multiple times or zoom into different regions as needed. Each \
time, use a <think> block to reason about what you observe. If the zoomed \
view contradicts your earlier reasoning, explicitly note what changed and why.

4. When you have gathered sufficient information to make your analysis, \
provide your final answer in an <answer> block.

Be natural and diverse in your reasoning. Explain WHY you make decisions and \
observations, not just WHAT you decide. Your reasoning should reflect genuine \
analysis of the visual evidence.

Do not include tool calls or thinking blocks in your final answer — only \
provide the answer block with your analysis.\
"""


# =============================================================================
# Utility Functions
# =============================================================================

def format_proposals(proposals: list[dict]) -> str:
    """Format proposal dicts as JSON for the prompt."""
    items = []
    for idx, p in enumerate(proposals):
        items.append({
            "id": idx,
            "bbox_2d": p["bbox_1000"],
            "label": p["class_name"],
            "confidence": round(p["confidence"], 2),
        })
    return json.dumps(items, indent=2)


def load_proposals(image_stem: str, proposals_dir: str) -> list[dict]:
    """Load proposal JSON for an image. Returns empty list if not found."""
    proposal_path = os.path.join(proposals_dir, f"{image_stem}.json")
    if not os.path.exists(proposal_path):
        return []
    with open(proposal_path) as f:
        data = json.load(f)
    return data.get("proposals", [])


def build_grounding_prompt(action: str, object_category: str, proposals: list[dict],
                           is_person_person: bool = False) -> str:
    """Build the user prompt for a grounding task."""
    proposals_text = format_proposals(proposals)
    obj = object_category

    parts = [
        "You will be performing a visual grounding task. You will be given "
        "a JSON array of candidate object proposals detected in an image, "
        "each with a bounding box (``bbox_2d`` in 1000x1000 normalized "
        "coordinates), label, and confidence score. "
        "Your goal is to identify specific objects and their spatial "
        "relationships based on the task description provided.\n\n"
        "Here are the candidate object proposals:\n\n"
        f"<candidate_objects>\n{proposals_text}\n</candidate_objects>\n\n"
        "Here is the task you need to complete:\n\n"
        "<task_description>\n"
        f"Locate every person who is **{action} {obj}** and the "
        f"**{obj}** they interact with in this image.\n"
        "</task_description>\n\n",
    ]

    if is_person_person:
        parts.append(
            "Note: In this interaction, both participants are people. "
            "The first bbox should be the **agent** (person performing the action) "
            "and the second should be the **patient** (person being acted upon).\n\n"
        )

    parts.append(
        "Your job is to carefully analyze the candidate objects and identify "
        "which ones satisfy the relationship or interaction described in the "
        "task. Pay close attention to:\n"
        "- Spatial proximity (are objects close enough to be interacting?)\n"
        "- Semantic relationships (does the pairing make sense for the "
        "described interaction?)\n"
        "- Object labels and their relevance to the task\n"
        "- **Objects visible in the image but missing from proposals** — the "
        "detector may not have found every relevant person or object. If you "
        "see additional interacting pairs in the image, estimate their "
        "bounding boxes from visual inspection and include them.\n\n"
        "Before providing your final answer, reason through your analysis "
        "step by step in a <think> block.\n\n"
        "Important formatting guidelines:\n"
        "- Bounding boxes use 1000x1000 normalized coordinates as "
        "[x1, y1, x2, y2]\n"
        "- **IMPORTANT: Each line must contain EXACTLY 2 objects** — one person "
        "and one interacted object.\n"
        "- If multiple pairs exist, include all of them\n"
        "- If no objects satisfy the task requirements, state this clearly\n\n"
        "Example output format for a single pair:\n"
        "<answer>\n"
        f'[{{"bbox_2d": [x1, y1, x2, y2], "label": "person"}}, '
        f'{{"bbox_2d": [x1, y1, x2, y2], "label": "{obj}"}}]\n'
        "</answer>\n\n"
        "Now, analyze the candidate objects and complete the task. "
        "Begin with your <think> reasoning, then provide your final "
        "answer inside <answer> tags."
    )

    return "".join(parts)


def image_to_base64(image: Image.Image) -> str:
    """Convert PIL Image to base64 data URL."""
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG", quality=90)
    img_b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
    return f"data:image/jpeg;base64,{img_b64}"


def execute_zoom_in(image: Image.Image, bbox_2d_1000: list) -> Image.Image:
    """Crop image at [0,1000] normalized bbox."""
    w, h = image.size
    x1 = max(0, int(bbox_2d_1000[0] * w / 1000))
    y1 = max(0, int(bbox_2d_1000[1] * h / 1000))
    x2 = min(w, int(bbox_2d_1000[2] * w / 1000))
    y2 = min(h, int(bbox_2d_1000[3] * h / 1000))
    # Ensure valid crop
    if x2 <= x1 or y2 <= y1:
        return image
    return image.crop((x1, y1, x2, y2))


def parse_tool_call(text: str) -> dict | None:
    """Extract tool call from model output."""
    match = re.search(r'<tool_call>\s*(.*?)\s*</tool_call>', text, re.DOTALL)
    if not match:
        return None
    try:
        return json.loads(match.group(1))
    except json.JSONDecodeError:
        return None


def extract_thinking(text: str) -> str:
    """Extract content from last <think> block."""
    matches = re.findall(r'<think>(.*?)</think>', text, re.DOTALL)
    return matches[-1].strip() if matches else ""


def extract_answer(text: str) -> str | None:
    """Extract content between <answer> and </answer>."""
    match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
    return match.group(1).strip() if match else None


def run_sft_agent_loop(client: OpenAI, model_name: str, messages: list,
                       original_image: Image.Image, max_turns: int = 5) -> tuple:
    """
    Multi-turn inference with zoom_in/zoom_out tool handling.

    Returns:
        (answer_text, tool_calls, thinking_text, zoom_crops)
        zoom_crops: list of (turn, bbox, PIL.Image) for each zoom_in call
    """
    current_image = original_image
    tool_calls_log = []
    all_thinking = []
    zoom_crops = []  # (turn, bbox, cropped_image)

    for turn in range(max_turns):
        # Build vision content for current turn
        current_messages = list(messages)
        # Replace image placeholder in last user message with current image
        last_user_msg = current_messages[-1]
        if last_user_msg["role"] == "user":
            content = last_user_msg["content"]
            if isinstance(content, list):
                # Already a list, update the image
                for item in content:
                    if item.get("type") == "image_url":
                        item["image_url"]["url"] = image_to_base64(current_image)
                        break

        max_tokens = 4096
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=current_messages,
                max_tokens=max_tokens,
                temperature=0.0,
            )
        except Exception as e:
            err_str = str(e)
            # Handle context overflow: extract available tokens from error and retry
            import re as _re
            m = _re.search(r'maximum context length is (\d+) tokens and your request has (\d+) input tokens', err_str)
            if m:
                available = int(m.group(1)) - int(m.group(2)) - 64  # 64 token buffer
                if available > 64:
                    print(f"\n  ⚠️  Context overflow (input too long), retrying with max_tokens={available}")
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=current_messages,
                        max_tokens=available,
                        temperature=0.0,
                    )
                else:
                    print(f"\n  ⚠️  Context overflow, insufficient space for response. Skipping turn.")
                    break
            else:
                raise
        text = response.choices[0].message.content

        # Extract thinking
        thinking = extract_thinking(text)
        if thinking:
            all_thinking.append(thinking)

        # Check for final answer
        answer = extract_answer(text)
        if answer is not None:
            return answer, tool_calls_log, "\n\n".join(all_thinking), zoom_crops

        # Check for tool call
        tool = parse_tool_call(text)
        if tool:
            tool_name = tool.get("name", "")
            tool_args = tool.get("arguments", {})

            if tool_name == "zoom_in":
                bbox = tool_args.get("bbox_2d", [0, 0, 1000, 1000])
                current_image = execute_zoom_in(original_image, bbox)
                tool_calls_log.append({"name": "zoom_in", "bbox": bbox, "turn": turn})
                zoom_crops.append((turn, bbox, current_image.copy()))
            elif tool_name == "zoom_out":
                current_image = original_image
                tool_calls_log.append({"name": "zoom_out", "turn": turn})

            # Append assistant message and user observation (zoomed image)
            messages.append({"role": "assistant", "content": text})
            messages.append({
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": image_to_base64(current_image)}
                    },
                    {
                        "type": "text",
                        "text": "Here is the zoomed view. Continue your analysis."
                    }
                ]
            })
        else:
            # No tool call and no answer — stop
            break

    return None, tool_calls_log, "\n\n".join(all_thinking), zoom_crops


def parse_grounding_answer(answer_text: str, img_width: int, img_height: int) -> list:
    """
    Parse SFT grounding answer into list of (person_bbox, object_bbox) pixel pairs.

    Answer format (one pair per line):
    [{"bbox_2d": [x1,y1,x2,y2], "label": "person"}, {"bbox_2d": [...], "label": "object"}]
    """
    if not answer_text or answer_text.strip().lower().startswith("no valid"):
        return []

    pairs = []
    for line in answer_text.strip().split("\n"):
        line = line.strip()
        if not line or line.startswith("//"):
            continue
        try:
            objects = json.loads(line)
            if not isinstance(objects, list) or len(objects) < 2:
                continue
            person_bbox_1000 = None
            object_bbox_1000 = None
            for obj in objects:
                label = obj.get("label", "").lower()
                bbox = obj.get("bbox_2d", [])
                if len(bbox) != 4:
                    continue
                if label == "person" and person_bbox_1000 is None:
                    person_bbox_1000 = bbox
                elif label != "person" and object_bbox_1000 is None:
                    object_bbox_1000 = bbox
            if person_bbox_1000 and object_bbox_1000:
                pairs.append({
                    "person_box": _bbox_1000_to_pixel(person_bbox_1000, img_width, img_height),
                    "object_box": _bbox_1000_to_pixel(object_bbox_1000, img_width, img_height),
                })
        except (json.JSONDecodeError, KeyError, TypeError):
            continue
    return pairs


def _bbox_1000_to_pixel(bbox: list, width: int, height: int) -> list:
    """Convert [0,1000] normalized bbox to pixel coordinates."""
    return [
        int(bbox[0] * width / 1000),
        int(bbox[1] * height / 1000),
        int(bbox[2] * width / 1000),
        int(bbox[3] * height / 1000),
    ]


def visualize_grounding(image_path, pred_pairs, gt_pairs, matches,
                        action, object_category, iou_threshold=0.5):
    """3-panel visualization: Predictions | Ground Truth | Overlay."""
    image = Image.open(image_path).convert("RGB")
    img_width, img_height = image.size
    pred_img = image.copy()
    gt_img = image.copy()
    overlay_img = image.copy()
    pred_draw = ImageDraw.Draw(pred_img)
    gt_draw = ImageDraw.Draw(gt_img)
    overlay_draw = ImageDraw.Draw(overlay_img)
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
        small_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
    except Exception:
        font = ImageFont.load_default()
        small_font = ImageFont.load_default()

    color_person_pred = (255, 0, 0)
    color_object_pred = (0, 0, 255)
    color_person_gt = (0, 200, 0)
    color_object_gt = (255, 200, 0)
    color_matched = (0, 200, 0)
    color_unmatched = (255, 0, 0)

    matched_pred_indices = {m[0] for m in matches}
    matched_gt_indices = {m[1] for m in matches}

    for idx, pair in enumerate(pred_pairs):
        pb, ob = pair["person_box"], pair["object_box"]
        is_matched = idx in matched_pred_indices
        pred_draw.rectangle(pb, outline=color_person_pred, width=3)
        pred_draw.text((pb[0], max(0, pb[1] - 15)), f"P{idx+1}", fill=color_person_pred, font=small_font)
        pred_draw.rectangle(ob, outline=color_object_pred, width=3)
        pred_draw.text((ob[0], max(0, ob[1] - 15)), f"O{idx+1}", fill=color_object_pred, font=small_font)
        pc = ((pb[0]+pb[2])/2, (pb[1]+pb[3])/2)
        oc = ((ob[0]+ob[2])/2, (ob[1]+ob[3])/2)
        pred_draw.line([pc, oc], fill=color_matched if is_matched else color_unmatched, width=2)

    for idx, pair in enumerate(gt_pairs):
        pb, ob = pair["person_box"], pair["object_box"]
        is_matched = idx in matched_gt_indices
        gt_draw.rectangle(pb, outline=color_person_gt, width=3)
        gt_draw.text((pb[0], max(0, pb[1] - 15)), f"P{idx+1}", fill=color_person_gt, font=small_font)
        gt_draw.rectangle(ob, outline=color_object_gt, width=3)
        gt_draw.text((ob[0], max(0, ob[1] - 15)), f"O{idx+1}", fill=color_object_gt, font=small_font)
        pc = ((pb[0]+pb[2])/2, (pb[1]+pb[3])/2)
        oc = ((ob[0]+ob[2])/2, (ob[1]+ob[3])/2)
        gt_draw.line([pc, oc], fill=color_matched if is_matched else color_unmatched, width=2)

    for m in matches:
        pred_idx, gt_idx = m[0], m[1]
        pp, gp = pred_pairs[pred_idx], gt_pairs[gt_idx]
        overlay_draw.rectangle(gp["person_box"], outline=(0, 200, 0), width=2)
        overlay_draw.rectangle(gp["object_box"], outline=(0, 200, 0), width=2)
        overlay_draw.rectangle(pp["person_box"], outline=(0, 100, 255), width=2)
        overlay_draw.rectangle(pp["object_box"], outline=(0, 100, 255), width=2)
        piou = calculate_iou(pp["person_box"], gp["person_box"])
        oiou = calculate_iou(pp["object_box"], gp["object_box"])
        overlay_draw.text((pp["person_box"][0], max(0, pp["person_box"][1] - 15)),
                          f"IoU:{(piou+oiou)/2:.2f}", fill=(255, 255, 255), font=small_font)
    for idx in range(len(pred_pairs)):
        if idx not in matched_pred_indices:
            pb, ob = pred_pairs[idx]["person_box"], pred_pairs[idx]["object_box"]
            overlay_draw.rectangle(pb, outline=(255, 0, 0), width=2)
            overlay_draw.rectangle(ob, outline=(255, 0, 0), width=2)
            overlay_draw.text((pb[0], max(0, pb[1] - 15)), "FP", fill=(255, 0, 0), font=small_font)
    for idx in range(len(gt_pairs)):
        if idx not in matched_gt_indices:
            pb, ob = gt_pairs[idx]["person_box"], gt_pairs[idx]["object_box"]
            overlay_draw.rectangle(pb, outline=(255, 165, 0), width=2)
            overlay_draw.rectangle(ob, outline=(255, 165, 0), width=2)
            overlay_draw.text((pb[0], max(0, pb[1] - 15)), "FN", fill=(255, 165, 0), font=small_font)

    header_h = 55
    total_w = img_width * 3
    final_img = Image.new("RGB", (total_w, img_height + header_h), color=(255, 255, 255))
    final_draw = ImageDraw.Draw(final_img)
    final_draw.text((img_width // 2 - 50, 8), "Predictions", fill=(0, 0, 0), font=font)
    final_draw.text((img_width + img_width // 2 - 55, 8), "Ground Truth", fill=(0, 0, 0), font=font)
    final_draw.text((2 * img_width + img_width // 2 - 35, 8), "Overlay", fill=(0, 0, 0), font=font)
    nm = len(matches)
    stats = (f"Action: {action} | Object: {object_category} | IoU≥{iou_threshold} | "
             f"Pred:{len(pred_pairs)} GT:{len(gt_pairs)} Matched:{nm} "
             f"FP:{len(pred_pairs)-nm} FN:{len(gt_pairs)-nm} "
             f"Recall:{nm/len(gt_pairs):.1%}" if gt_pairs else "No GT pairs")
    final_draw.text((10, 32), stats, fill=(0, 0, 0), font=small_font)
    final_img.paste(pred_img, (0, header_h))
    final_img.paste(gt_img, (img_width, header_h))
    final_img.paste(overlay_img, (2 * img_width, header_h))
    return final_img


def calculate_iou(box1: list, box2: list) -> float:
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2
    inter_x1 = max(x1_1, x1_2)
    inter_y1 = max(y1_1, y1_2)
    inter_x2 = min(x2_1, x2_2)
    inter_y2 = min(y2_1, y2_2)
    if inter_x2 < inter_x1 or inter_y2 < inter_y1:
        return 0.0
    inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)
    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)
    union_area = box1_area + box2_area - inter_area
    return inter_area / union_area if union_area > 0 else 0.0


def pair_iou(pred: dict, gt: dict) -> float:
    """Combined IoU for a person-object pair (min of the two IoUs)."""
    iou_person = calculate_iou(pred["person_box"], gt["person_box"])
    iou_object = calculate_iou(pred["object_box"], gt["object_box"])
    return min(iou_person, iou_object)


def match_pairs_greedy(pred_pairs: list, gt_pairs: list, iou_threshold: float = 0.5):
    """Greedily match predicted pairs to GT pairs at given IoU threshold."""
    if not pred_pairs or not gt_pairs:
        return [], list(range(len(pred_pairs))), list(range(len(gt_pairs)))

    matched = []
    used_preds = set()
    used_gts = set()

    iou_matrix = [[pair_iou(p, g) for g in gt_pairs] for p in pred_pairs]

    # Greedy: match by highest IoU first
    scores = []
    for pi, row in enumerate(iou_matrix):
        for gi, iou in enumerate(row):
            if iou >= iou_threshold:
                scores.append((iou, pi, gi))
    scores.sort(reverse=True)

    for iou, pi, gi in scores:
        if pi not in used_preds and gi not in used_gts:
            matched.append((pi, gi))
            used_preds.add(pi)
            used_gts.add(gi)

    unmatched_preds = [i for i in range(len(pred_pairs)) if i not in used_preds]
    unmatched_gts = [i for i in range(len(gt_pairs)) if i not in used_gts]
    return matched, unmatched_preds, unmatched_gts


def categorize_pair_by_size(gt_pair: dict, area_small: float = 1024, area_medium: float = 9216) -> str:
    """Categorize GT pair by object size using COCO-style absolute pixel thresholds.
    Small: object < 32**2 pixels²
    Medium: 32**2 to 96**2 pixels²
    Large: >= 96**2 pixels²
    """
    o = gt_pair["object_box"]
    object_area = max(0, o[2] - o[0]) * max(0, o[3] - o[1])
    if object_area < area_small:
        return "small"
    elif object_area < area_medium:
        return "medium"
    return "large"


# =============================================================================
# Main Evaluation
# =============================================================================

def eval_model(args):
    print("=" * 80)
    print("HICO-DET Grounding Evaluation (SFT Qwen3VL + vLLM)")
    print("=" * 80)
    print(f"vLLM URL:    {args.vllm_url}")
    print(f"Model:       {args.model_name}")
    print(f"Annotation:  {args.ann_file}")
    print(f"Images:      {args.img_prefix}")
    print(f"Proposals:   {args.proposals_dir}")
    print(f"Output:      {args.result_file}")
    if args.max_images:
        print(f"Max images:  {args.max_images} (DEBUGGING MODE)")
    print("=" * 80)
    print()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Initialize vLLM client
    client = OpenAI(base_url=f"{args.vllm_url}/v1", api_key="placeholder")

    # Initialize W&B
    use_wandb = WANDB_AVAILABLE and args.wandb
    if use_wandb:
        try:
            wandb.login()
            wandb.init(
                project=args.wandb_project,
                name=args.wandb_run_name or f"hico_ground_sft_{timestamp}",
                config={
                    "model": args.model_name,
                    "vllm_url": args.vllm_url,
                    "dataset": "HICO-DET-Ground",
                    "task": "grounding",
                    "max_images": args.max_images,
                },
                tags=["hico", "grounding", "sft", "tool-use"]
            )
        except Exception as e:
            print(f"Warning: WandB init failed: {e}")
            use_wandb = False

    # Load annotations
    print(f"Loading annotations from: {args.ann_file}")
    with open(args.ann_file) as f:
        dataset_samples = json.load(f)
    print(f"Loaded {len(dataset_samples)} samples")

    if args.max_images:
        dataset_samples = dataset_samples[:args.max_images]
    print(f"Evaluating {len(dataset_samples)} samples")
    print("=" * 80)

    # Metrics storage
    iou_thresholds = [round(0.5 + 0.05 * i, 2) for i in range(10)]

    results_per_threshold = {
        iou_thr: {"tp": 0, "fp": 0, "fn": 0,
                  "tp_small": 0, "fn_small": 0,
                  "tp_medium": 0, "fn_medium": 0,
                  "tp_large": 0, "fn_large": 0}
        for iou_thr in iou_thresholds
    }

    per_sample_results = []
    action_stats = defaultdict(lambda: {
        "total_samples": 0, "total_gt_pairs": 0,
        "total_pred_pairs": 0, "matched_pairs_05": 0
    })
    thinking_records = []
    missing_proposals = 0

    # Resume support: load previously completed samples from partial checkpoint
    partial_file = args.result_file + ".partial.jsonl"
    processed_indices: set = set()
    if args.resume and os.path.exists(partial_file):
        print(f"\nResuming from partial checkpoint: {partial_file}")
        loaded = 0
        with open(partial_file) as _pf:
            for line in _pf:
                line = line.strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    processed_indices.add(rec["idx"])
                    per_sample_results.append(rec["sample_result"])
                    for iou_thr_str, contrib in rec["per_iou"].items():
                        iou_thr = float(iou_thr_str)
                        for key, val in contrib.items():
                            results_per_threshold[iou_thr][key] += val
                    action = rec["action_key"]
                    for key, val in rec["action_update"].items():
                        action_stats[action][key] += val
                    missing_proposals += rec["missing_proposal"]
                    if rec.get("thinking_record"):
                        thinking_records.append(rec["thinking_record"])
                    loaded += 1
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"  Warning: skipping corrupt partial record: {e}")
        print(f"Loaded {loaded} completed samples, resuming from next unprocessed...")
    elif args.resume:
        print(f"No partial checkpoint found at {partial_file}, starting fresh")
    partial_f = open(partial_file, "a")

    show_verbose = args.verbose or len(dataset_samples) <= 100

    viz_dir = None
    if show_verbose:
        viz_dir = args.result_file.replace(".json", "_visualizations")
        os.makedirs(viz_dir, exist_ok=True)
        print(f"Visualization directory: {viz_dir}\n")

    print("\nStarting evaluation...")
    for idx, sample in enumerate(tqdm(dataset_samples, disable=show_verbose)):
        if idx in processed_indices:
            continue
        file_name = sample["file_name"]
        action = sample["action"]
        object_category = sample["object_category"]
        is_person_person = sample.get("is_person_person", False)
        img_path = os.path.join(args.img_prefix, file_name)
        img_width = sample["width"]
        img_height = sample["height"]

        # Build GT pairs using gt_box_inds
        boxes = sample["boxes"]
        gt_box_inds = sample["gt_box_inds"]
        num_pairs = sample["num_pairs"]
        gt_pairs = []
        for i in range(num_pairs):
            person_idx = gt_box_inds[i * 2]
            object_idx = gt_box_inds[i * 2 + 1]
            gt_pairs.append({
                "person_box": boxes[person_idx],
                "object_box": boxes[object_idx],
            })

        # Load proposals
        image_stem = os.path.splitext(file_name)[0]
        proposals = load_proposals(image_stem, args.proposals_dir)
        if not proposals:
            missing_proposals += 1

        if show_verbose:
            print(f"\n[{idx+1}/{len(dataset_samples)}] {file_name}")
            print(f"  Action: {action}, Object: {object_category}, Pairs: {num_pairs}")
            print(f"  Proposals: {len(proposals)}")

        # Load image
        image = Image.open(img_path).convert("RGB")

        # Build initial messages
        user_prompt = build_grounding_prompt(action, object_category, proposals)
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_to_base64(image)}},
                    {"type": "text", "text": user_prompt},
                ]
            }
        ]

        # Run agent loop
        answer_text, tool_calls, thinking, zoom_crops = run_sft_agent_loop(
            client, args.model_name, messages, image, max_turns=args.max_turns
        )

        if show_verbose:
            print(f"  Tool calls: {len(tool_calls)}")
            if thinking:
                print(f"  Thinking: {thinking[:150]}...")
            print(f"  Answer: {str(answer_text)[:100]}")

        # Parse answer
        pred_pairs = parse_grounding_answer(answer_text or "", img_width, img_height)

        if show_verbose:
            print(f"  Predicted pairs: {len(pred_pairs)}, GT pairs: {len(gt_pairs)}")

        # Save thinking
        if thinking:
            thinking_records.append({
                "file_name": file_name,
                "action": action,
                "object": object_category,
                "thinking": thinking,
                "tool_calls": tool_calls,
                "answer": answer_text,
            })

        # Update action stats
        action_stats[action]["total_samples"] += 1
        action_stats[action]["total_gt_pairs"] += len(gt_pairs)
        action_stats[action]["total_pred_pairs"] += len(pred_pairs)

        # Match at each IoU threshold
        sample_result = {
            "file_name": file_name,
            "action": action,
            "object": object_category,
            "num_gt_pairs": len(gt_pairs),
            "num_pred_pairs": len(pred_pairs),
            "tool_calls": tool_calls,
            "answer": answer_text,
            "matches_per_threshold": {},
        }

        sample_iou_contribs = {}
        matched_05 = 0
        for iou_thr in iou_thresholds:
            matches, unmatched_preds, unmatched_gts = match_pairs_greedy(
                pred_pairs, gt_pairs, iou_threshold=iou_thr
            )
            tp = len(matches)
            fp = len(unmatched_preds)
            fn = len(unmatched_gts)
            results_per_threshold[iou_thr]["tp"] += tp
            results_per_threshold[iou_thr]["fp"] += fp
            results_per_threshold[iou_thr]["fn"] += fn

            matched_gt_indices = {m[1] for m in matches}
            tp_small = fn_small = tp_medium = fn_medium = tp_large = fn_large = 0
            for gt_idx, gt_pair in enumerate(gt_pairs):
                AREA_SMALL = 32 ** 2    # < 1024 pixels²
                AREA_MEDIUM = 96 ** 2   # < 9216 pixels²
                size_cat = categorize_pair_by_size(gt_pair, AREA_SMALL, AREA_MEDIUM)
                if gt_idx in matched_gt_indices:
                    results_per_threshold[iou_thr][f"tp_{size_cat}"] += 1
                    if size_cat == "small": tp_small += 1
                    elif size_cat == "medium": tp_medium += 1
                    else: tp_large += 1
                else:
                    results_per_threshold[iou_thr][f"fn_{size_cat}"] += 1
                    if size_cat == "small": fn_small += 1
                    elif size_cat == "medium": fn_medium += 1
                    else: fn_large += 1

            sample_iou_contribs[str(iou_thr)] = {
                "tp": tp, "fp": fp, "fn": fn,
                "tp_small": tp_small, "fn_small": fn_small,
                "tp_medium": tp_medium, "fn_medium": fn_medium,
                "tp_large": tp_large, "fn_large": fn_large,
            }

            sample_result["matches_per_threshold"][iou_thr] = {
                "matched": tp,
                "unmatched_preds": fp,
                "unmatched_gts": fn,
            }

            if iou_thr == 0.5:
                action_stats[action]["matched_pairs_05"] += tp
                matched_05 = tp

        per_sample_results.append(sample_result)

        # Save partial checkpoint for resume capability
        partial_rec = {
            "idx": idx,
            "sample_result": sample_result,
            "per_iou": sample_iou_contribs,
            "action_key": action,
            "action_update": {
                "total_samples": 1,
                "total_gt_pairs": len(gt_pairs),
                "total_pred_pairs": len(pred_pairs),
                "matched_pairs_05": matched_05,
            },
            "missing_proposal": 1 if not proposals else 0,
            "thinking_record": thinking_records[-1] if thinking else None,
        }
        partial_f.write(json.dumps(partial_rec) + "\n")
        partial_f.flush()

        # Visualization (verbose mode only)
        if viz_dir is not None:
            matches_05, _, _ = match_pairs_greedy(pred_pairs, gt_pairs, iou_threshold=0.5)
            base_name = os.path.splitext(file_name)[0]
            action_safe = action.replace(" ", "_").replace("/", "_")
            object_safe = object_category.replace(" ", "_").replace("/", "_")
            prefix = f"{base_name}_{action_safe}_{object_safe}"
            try:
                viz_img = visualize_grounding(
                    img_path, pred_pairs, gt_pairs, matches_05,
                    action, object_category, iou_threshold=0.5
                )
                viz_path = os.path.join(viz_dir, f"{prefix}_viz.jpg")
                viz_img.save(viz_path, quality=90)
                if show_verbose:
                    print(f"  Visualization: {prefix}_viz.jpg")
            except Exception as e:
                if show_verbose:
                    print(f"  Visualization failed: {e}")
            # Save zoom-in crops
            for crop_turn, crop_bbox, crop_img in zoom_crops:
                try:
                    crop_path = os.path.join(viz_dir, f"{prefix}_turn{crop_turn}_zoomin.jpg")
                    crop_img.save(crop_path, quality=90)
                    if show_verbose:
                        print(f"  Zoom crop turn{crop_turn}: bbox={crop_bbox}")
                except Exception as e:
                    if show_verbose:
                        print(f"  Zoom crop save failed (turn {crop_turn}): {e}")

    partial_f.close()

    # Compute AR metrics
    print("\n" + "=" * 80)
    print("HICO-DET Grounding Results (SFT Qwen3VL)")
    print("=" * 80)

    recalls = []
    recalls_small, recalls_medium, recalls_large = [], [], []

    for iou_thr in iou_thresholds:
        tp = results_per_threshold[iou_thr]["tp"]
        fn = results_per_threshold[iou_thr]["fn"]
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        recalls.append(recall)

        tp_s = results_per_threshold[iou_thr]["tp_small"]
        fn_s = results_per_threshold[iou_thr]["fn_small"]
        recalls_small.append(tp_s / (tp_s + fn_s) if (tp_s + fn_s) > 0 else 0.0)

        tp_m = results_per_threshold[iou_thr]["tp_medium"]
        fn_m = results_per_threshold[iou_thr]["fn_medium"]
        recalls_medium.append(tp_m / (tp_m + fn_m) if (tp_m + fn_m) > 0 else 0.0)

        tp_l = results_per_threshold[iou_thr]["tp_large"]
        fn_l = results_per_threshold[iou_thr]["fn_large"]
        recalls_large.append(tp_l / (tp_l + fn_l) if (tp_l + fn_l) > 0 else 0.0)

    ar = float(np.mean(recalls)) if recalls else 0.0
    ar_50 = recalls[0] if recalls else 0.0
    ar_75 = recalls[5] if len(recalls) > 5 else 0.0
    ar_small = float(np.mean(recalls_small))
    ar_medium = float(np.mean(recalls_medium))
    ar_large = float(np.mean(recalls_large))

    metrics = {
        "AR": ar, "AR@0.5": ar_50, "AR@0.75": ar_75,
        "ARs": ar_small, "ARm": ar_medium, "ARl": ar_large,
        "missing_proposals": missing_proposals,
    }

    print(f"{'AR':<12} {ar*100:>9.1f}%  Average Recall @ IoU=0.50:0.95")
    print(f"{'AR@0.5':<12} {ar_50*100:>9.1f}%  Average Recall @ IoU=0.50")
    print(f"{'AR@0.75':<12} {ar_75*100:>9.1f}%  Average Recall @ IoU=0.75")
    print(f"{'ARs':<12} {ar_small*100:>9.1f}%  Small objects")
    print(f"{'ARm':<12} {ar_medium*100:>9.1f}%  Medium objects")
    print(f"{'ARl':<12} {ar_large*100:>9.1f}%  Large objects")
    print(f"\nMissing proposals: {missing_proposals}/{len(dataset_samples)}")

    # Save results
    os.makedirs(os.path.dirname(os.path.abspath(args.result_file)), exist_ok=True)

    print(f"\nSaving results to: {args.result_file}")
    with open(args.result_file, "w") as f:
        json.dump(per_sample_results, f, indent=2)

    metrics_file = args.result_file.replace(".json", "_metrics.json")
    with open(metrics_file, "w") as f:
        json.dump(metrics, f, indent=2)

    action_stats_file = args.result_file.replace(".json", "_action_stats.json")
    with open(action_stats_file, "w") as f:
        json.dump(dict(action_stats), f, indent=2)

    if thinking_records:
        thinking_file = args.result_file.replace(".json", "_thinking.jsonl")
        with open(thinking_file, "w") as f:
            for r in thinking_records:
                f.write(json.dumps(r) + "\n")
        print(f"Thinking saved: {thinking_file} ({len(thinking_records)} samples)")

    # Remove partial checkpoint on successful completion
    if os.path.exists(partial_file):
        os.remove(partial_file)
        print(f"Partial checkpoint removed: {partial_file}")

    if use_wandb:
        wandb.log(metrics)
        wandb.finish()

    print("\nEvaluation complete!")
    return metrics


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="HICO-DET SFT Grounding Evaluation")
    parser.add_argument("--vllm-url", type=str, default="http://localhost:8000",
                        help="vLLM server URL")
    parser.add_argument("--model-name", type=str, default="qwen3VL-4B",
                        help="Model name as registered in vLLM")
    parser.add_argument("--ann-file", type=str,
                        default="../dataset/benchmarks_simplified/hico_ground_test_simplified.json",
                        help="Path to HICO grounding annotation file")
    parser.add_argument("--img-prefix", type=str,
                        default="../dataset/hico_20160224_det/images/test2015",
                        help="Path to HICO images directory")
    parser.add_argument("--proposals-dir", type=str,
                        default="../../hoi-dataset-curation/output/test_proposals",
                        help="Path to directory containing YOLOE proposal JSON files")
    parser.add_argument("--result-file", type=str, required=True,
                        help="Output file for evaluation results")
    parser.add_argument("--max-images", type=int, default=None,
                        help="Limit evaluation to first N samples")
    parser.add_argument("--max-turns", type=int, default=5,
                        help="Maximum tool call turns per sample")
    parser.add_argument("--verbose", action="store_true",
                        help="Show per-sample results")
    parser.add_argument("--wandb", action="store_true",
                        help="Enable Weights & Biases logging")
    parser.add_argument("--wandb-project", type=str, default="hico-grounding-sft",
                        help="W&B project name")
    parser.add_argument("--wandb-run-name", type=str, default=None,
                        help="W&B run name")
    parser.add_argument("--resume", action="store_true",
                        help="Resume from partial checkpoint if available")

    args = parser.parse_args()
    eval_model(args)
